{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098a6d92-e313-4152-b6de-6d64f60b6483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.2636 - loss: 1.6078 - val_accuracy: 0.3000 - val_loss: 1.5937\n",
      "Epoch 2/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3974 - loss: 1.5786 - val_accuracy: 0.3000 - val_loss: 1.5730\n",
      "Epoch 3/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4600 - loss: 1.5153 - val_accuracy: 0.4750 - val_loss: 1.4971\n",
      "Epoch 4/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6642 - loss: 1.3307 - val_accuracy: 0.6250 - val_loss: 1.2414\n",
      "Epoch 5/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6870 - loss: 1.0093 - val_accuracy: 0.7250 - val_loss: 1.1371\n",
      "Epoch 6/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7960 - loss: 0.8678 - val_accuracy: 0.7500 - val_loss: 1.0964\n",
      "Epoch 7/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8590 - loss: 0.6470 - val_accuracy: 0.6000 - val_loss: 1.1672\n",
      "Epoch 8/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9321 - loss: 0.3951 - val_accuracy: 0.6250 - val_loss: 1.1225\n",
      "Epoch 9/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9362 - loss: 0.3018 - val_accuracy: 0.6500 - val_loss: 1.1245\n",
      "Epoch 10/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9745 - loss: 0.1881 - val_accuracy: 0.6500 - val_loss: 1.1902\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6162 - loss: 1.0473 \n",
      "Test Accuracy: 0.5899999737739563\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Test Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define constants\n",
    "MAX_NB_WORDS = 5000  # Maximum number of words to tokenize\n",
    "MAX_SEQUENCE_LENGTH = 100  # Maximum length of each sequence\n",
    "EMBEDDING_DIM = 100  # Dimension of word embeddings\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "labels = []\n",
    "folders = ['alone', 'dance', 'death', 'funny', 'god']\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join('topics', folder)\n",
    "    for file in os.listdir(folder_path):\n",
    "        with open(os.path.join(folder_path, file), 'r') as f:\n",
    "            data.append(f.read())\n",
    "            labels.append(folder)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(len(folders), activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "# Predictions\n",
    "y_pred_prob = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6376da6f-06d7-44fd-ac35-2df036ac3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the model parameters\n",
    "MAX_NB_WORDS = 5000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the embedding layer separately\n",
    "embedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM)\n",
    "\n",
    "# CNN Model\n",
    "cnn_model = Sequential([\n",
    "    embedding_layer,\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Flatten(),\n",
    "    Dense(len(folders), activation='softmax')\n",
    "])\n",
    "\n",
    "# Specify the input shape\n",
    "cnn_model.build(input_shape=(None, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "cnn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e092418b-a34c-422d-9ffa-33ce119c9c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1840 - loss: 1.6120 - val_accuracy: 0.1500 - val_loss: 1.7013\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4559 - loss: 1.4864 - val_accuracy: 0.1700 - val_loss: 1.6230\n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7823 - loss: 1.3668 - val_accuracy: 0.1900 - val_loss: 1.6709\n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8116 - loss: 1.2109 - val_accuracy: 0.2900 - val_loss: 1.5884\n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9107 - loss: 0.9838 - val_accuracy: 0.3900 - val_loss: 1.4715\n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9529 - loss: 0.6767 - val_accuracy: 0.5400 - val_loss: 1.2920\n",
      "Epoch 7/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9546 - loss: 0.4082 - val_accuracy: 0.6100 - val_loss: 1.1118\n",
      "Epoch 8/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9949 - loss: 0.2472 - val_accuracy: 0.6700 - val_loss: 0.9873\n",
      "Epoch 9/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9956 - loss: 0.1382 - val_accuracy: 0.6400 - val_loss: 0.9185\n",
      "Epoch 10/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9949 - loss: 0.0791 - val_accuracy: 0.6600 - val_loss: 0.8817\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6588 - loss: 0.9153\n",
      "Test Loss: 0.8816856741905212\n",
      "Test Accuracy: 0.6600000262260437\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = cnn_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = cnn_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22a8042f-5d90-4875-a3bf-6a3b78e0f331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.2221 - loss: 1.6066 - val_accuracy: 0.2500 - val_loss: 1.5936\n",
      "Epoch 2/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.2571 - loss: 1.5669 - val_accuracy: 0.3000 - val_loss: 1.5535\n",
      "Epoch 3/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.4982 - loss: 1.4366 - val_accuracy: 0.4750 - val_loss: 1.4057\n",
      "Epoch 4/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.7119 - loss: 1.1739 - val_accuracy: 0.6750 - val_loss: 1.2886\n",
      "Epoch 5/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.7687 - loss: 0.8885 - val_accuracy: 0.6250 - val_loss: 1.2548\n",
      "Epoch 6/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.8558 - loss: 0.6326 - val_accuracy: 0.7250 - val_loss: 1.1915\n",
      "Epoch 7/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.9075 - loss: 0.4973 - val_accuracy: 0.7250 - val_loss: 1.1389\n",
      "Epoch 8/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.9444 - loss: 0.3274 - val_accuracy: 0.7750 - val_loss: 1.0692\n",
      "Epoch 9/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step - accuracy: 0.9846 - loss: 0.2102 - val_accuracy: 0.7500 - val_loss: 1.0660\n",
      "Epoch 10/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step - accuracy: 0.9668 - loss: 0.1543 - val_accuracy: 0.7500 - val_loss: 1.1781\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 287ms/step - accuracy: 0.7276 - loss: 0.7645\n",
      "Bidirectional LSTM Test Accuracy: 0.7200000286102295\n",
      "Epoch 1/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.1973 - loss: 1.6521 - val_accuracy: 0.2250 - val_loss: 1.6046\n",
      "Epoch 2/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.3150 - loss: 1.5430 - val_accuracy: 0.2750 - val_loss: 1.5722\n",
      "Epoch 3/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.4023 - loss: 1.4246 - val_accuracy: 0.3000 - val_loss: 1.5539\n",
      "Epoch 4/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.6322 - loss: 1.3177 - val_accuracy: 0.3750 - val_loss: 1.5010\n",
      "Epoch 5/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.7249 - loss: 1.0869 - val_accuracy: 0.5000 - val_loss: 1.4512\n",
      "Epoch 6/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.8623 - loss: 0.7988 - val_accuracy: 0.5750 - val_loss: 1.3161\n",
      "Epoch 7/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 0.9157 - loss: 0.4676 - val_accuracy: 0.6250 - val_loss: 1.2970\n",
      "Epoch 8/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.9820 - loss: 0.2623 - val_accuracy: 0.6500 - val_loss: 1.2556\n",
      "Epoch 9/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.9947 - loss: 0.1378 - val_accuracy: 0.6250 - val_loss: 1.2344\n",
      "Epoch 10/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.9978 - loss: 0.0831 - val_accuracy: 0.6500 - val_loss: 1.3173\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6942 - loss: 0.9563\n",
      "CNN Test Accuracy: 0.7200000286102295\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "labels = []\n",
    "folders = ['alone', 'dance', 'death', 'funny', 'god']\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join('topics', folder)\n",
    "    for file in os.listdir(folder_path):\n",
    "        with open(os.path.join(folder_path, file), 'r') as f:\n",
    "            data.append(f.read())\n",
    "            labels.append(folder)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model 1: Bidirectional LSTM\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(len(word_index) + 1, 100))\n",
    "model_lstm.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model_lstm.add(Dense(len(folders), activation='softmax'))\n",
    "\n",
    "# Compile and train the Bidirectional LSTM model\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "model_lstm.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.1)\n",
    "end_time = time.time()\n",
    "train_time_lstm = end_time - start_time\n",
    "\n",
    "# Evaluate the Bidirectional LSTM model\n",
    "start_time = time.time()\n",
    "loss_lstm, accuracy_lstm = model_lstm.evaluate(x_test, y_test)\n",
    "end_time = time.time()\n",
    "test_time_lstm = end_time - start_time\n",
    "\n",
    "print(\"Bidirectional LSTM Test Accuracy:\", accuracy_lstm)\n",
    "\n",
    "# Model 2: CNN\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(len(word_index) + 1, 100))\n",
    "model_cnn.add(Conv1D(128, 5, activation='relu'))\n",
    "model_cnn.add(MaxPooling1D(5))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(len(folders), activation='softmax'))\n",
    "\n",
    "# Compile and train the CNN model\n",
    "model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "model_cnn.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.1)\n",
    "end_time = time.time()\n",
    "train_time_cnn = end_time - start_time\n",
    "\n",
    "# Evaluate the CNN model\n",
    "start_time = time.time()\n",
    "loss_cnn, accuracy_cnn = model_cnn.evaluate(x_test, y_test)\n",
    "end_time = time.time()\n",
    "test_time_cnn = end_time - start_time\n",
    "\n",
    "print(\"CNN Test Accuracy:\", accuracy_cnn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb73fbb0-a2e2-4768-a91c-1afdda1f8fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.1879 - loss: 43.3644 - val_accuracy: 0.2250 - val_loss: 56.8644\n",
      "Epoch 2/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.2058 - loss: 26.1236 - val_accuracy: 0.1750 - val_loss: 4.9454\n",
      "Epoch 3/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.1600 - loss: 2.9849 - val_accuracy: 0.1750 - val_loss: 1.6100\n",
      "Epoch 4/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.1792 - loss: 1.6692 - val_accuracy: 0.1750 - val_loss: 1.6100\n",
      "Epoch 5/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.1638 - loss: 1.5994 - val_accuracy: 0.1750 - val_loss: 1.6100\n",
      "Epoch 6/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.1843 - loss: 1.6059 - val_accuracy: 0.1750 - val_loss: 1.6099\n",
      "Epoch 7/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.1564 - loss: 1.6049 - val_accuracy: 0.1750 - val_loss: 1.6098\n",
      "Epoch 8/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.1812 - loss: 1.6051 - val_accuracy: 0.1750 - val_loss: 1.6099\n",
      "Epoch 9/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.2138 - loss: 1.5975 - val_accuracy: 0.1750 - val_loss: 1.6098\n",
      "Epoch 10/10\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.2248 - loss: 1.6005 - val_accuracy: 0.1750 - val_loss: 1.6098\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0890 - loss: 1.6103 \n",
      "Feedforward Neural Network Test Accuracy: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Model 4: Feedforward Neural Network\n",
    "model_ffnn = Sequential()\n",
    "model_ffnn.add(Embedding(len(word_index) + 1, 100))\n",
    "model_ffnn.add(Flatten())\n",
    "model_ffnn.add(Dense(64, activation='relu'))\n",
    "model_ffnn.add(Dense(len(folders), activation='softmax'))\n",
    "\n",
    "# Compile and train the Feedforward Neural Network model\n",
    "model_ffnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "model_ffnn.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.1)\n",
    "end_time = time.time()\n",
    "train_time_ffnn = end_time - start_time\n",
    "\n",
    "# Evaluate the Feedforward Neural Network model\n",
    "start_time = time.time()\n",
    "loss_ffnn, accuracy_ffnn = model_ffnn.evaluate(x_test, y_test)\n",
    "end_time = time.time()\n",
    "test_time_ffnn = end_time - start_time\n",
    "\n",
    "print(\"Feedforward Neural Network Test Accuracy:\", accuracy_ffnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f8f90a2-2238-4c0e-835d-90f7203f3436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 138ms/step\n",
      "BERT Test Accuracy: 0.0005555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Model 3: BERT\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "x_train_bert = tokenizer_bert(\n",
    "    [str(text) for text in x_train],  # Convert to list of strings\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "x_test_bert = tokenizer_bert(\n",
    "    [str(text) for text in x_test],  # Convert to list of strings\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "model_bert = TFBertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "\n",
    "# Evaluate the BERT model\n",
    "start_time = time.time()\n",
    "outputs = model_bert.predict(x_test_bert)\n",
    "end_time = time.time()\n",
    "test_time_bert = end_time - start_time\n",
    "\n",
    "# Get predicted labels\n",
    "predicted_labels = tf.argmax(outputs.last_hidden_state, axis=-1)\n",
    "\n",
    "# Flatten the predicted labels if necessary\n",
    "predicted_labels_flat = tf.reshape(predicted_labels, [-1])\n",
    "\n",
    "# Reshape y_test to match the shape of predicted_labels_flat\n",
    "y_test_reshaped = np.repeat(y_test, predicted_labels_flat.shape[0] // len(y_test))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_bert = np.mean(predicted_labels_flat.numpy() == y_test_reshaped)\n",
    "\n",
    "print(\"BERT Test Accuracy:\", accuracy_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15897d-e12a-4e4e-8476-387054083511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
